# ThreadPoolExecutor 参数详解及作用

### corePoolSize  -  核心线程数

核心线程会一直存活，即使没有任务需要执行， 当线程数小于核心线程数时，即使有线程空闲，线程池也会优先创建新线程处理，设置allowCoreThreadTimeout=true（默认false）时，核心线程会超时关闭

### queueCapacity  -  任务队列容量(阻塞队列)

当核心线程数达到最大时，新任务会放在队列中排队执行。

### maxPoolSize  -  最大线程数

当线程数 >= corePoolSize，且任务队列已满时，线程池会创建新线程来处理任务，当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常。

``` java
if (currentThreadCount >= corePoolSize) {
    queue.offer(waitHandleTask);
}
if (currentThreadCount >= maxPoolSize && queue.isFull()) {
    rejectTask(exception);
}
```

### keepAliveTime  -  线程空闲时间

线程空闲时间达到keepAliveTime时，线程会退出，直到线程数量=corePoolSize

如果allowCoreThreadTimeout=true（核心线程超时关闭），则会直到线程数量=0



### allowCoreThreadTimeout  -  允许核心线程超时

### rejectExecutionHandler  -  任务拒绝处理器

* 两种情况会拒绝处理任务：
            - 当线程数已经达到maxPoolSize，且队列已满，会拒绝新任务
                - 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务
* 线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常
* ThreadPoolExecutor类有几个内部实现类来处理这类情况：
	- AbortPolicy - 丢弃任务，抛运行时异常 -> RejectedExecutionException
	- CallerRunsPolicy - 直接执行任务
	- DiscardPolicy - 忽视，什么都不会发生
	- DiscardOldestPolicy - 从队列中踢出最先进入队列（最后一个执行）的任务
* 实现RejectedExecutionHandler接口，可自定义处理器

## 如何设置参数

1、默认值

* corePoolSize=1
* queueCapacity=Integer.MAX_VALUE
* maxPoolSize=Integer.MAX_VALUE
* keepAliveTime=60s
* allowCoreThreadTimeout=false
* rejectedExecutionHandler=AbortPolicy()

2、如何来设置

* 需要根据几个值来决定
	- tasks ：每秒的任务数，假设为500~1000
	- taskcost：每个任务花费时间，假设为0.1s
	- responsetime：系统允许容忍的最大响应时间，假设为1s
	
* 做几个计算
	
	- corePoolSize = 每秒需要多少个线程处理？ 
	
* $$
  threadcount = tasks / ( 1 / taskcost ) = tasks * taskcout
  $$
  = (500~1000) * 0.1 = 50~100 个线程。corePoolSize设置应该大于50

* 根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可
	
	- queueCapacity = (coreSizePool/taskcost)*responsetime
	
* 计算可得 queueCapacity = 80/0.1*1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行

* 切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。
	
	- maxPoolSize = (max(tasks)- queueCapacity)/(1/taskcost)
	
* 计算可得 maxPoolSize = (1000-80)/10 = 92

* （最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数
	- rejectedExecutionHandler：根据具体情况来决定，任务不重要可丢弃，任务重要则要利用一些缓冲机制来处理
	- keepAliveTime和allowCoreThreadTimeout采用默认通常能满足

3、 以上都是理想值，实际情况下要根据机器性能来决定。如果在未达到最大线程数的情况机器cpu load已经满了，则需要通过升级硬件（呵呵）和优化代码，降低taskcost来处理。



# MySQL 常用面试题

## 事务隔离

https://zhuanlan.zhihu.com/p/117476959

- Read_UnCommitted  -  读未提交，如果A事务执行更新，但未提交，事务B能看到A执行的数据，此时B拿到脏数据去执行其他操作，A事务由于某些原因执行回滚，则数据是错误的。
- Read-Committed  -  读提交，A事务执行更新，但未提交，事务B看不到A执行的数据，等A事务提交后，B才能拿到更新后的数据。在同一事务中(本例中的事务B)，事务的不同时刻同样的查询条件，查询出来的记录内容是不一样的，事务A的提交影响了事务B的查询结果，这就是不可重复读，也就是读提交隔离级别。每个 select 语句都有自己的一份快照，而不是一个事务一份，所以在不同的时刻，查询出来的数据可能是不一致的。在读提交解决了脏读的问题，但是无法做到可重复读，也没办法解决幻读。
- Repeatable_Read  -  可重复读
- Serializable  -  串行化

## 事务的并发问题

**脏读**

脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并不一定最终存在的数据，这就是脏读。

**可重复读**

可重复读指的是在一个事务内，最开始读到的数据和事务结束前的任意时刻读到的同一批数据都是一致的。通常针对数据**更新（UPDATE）**操作。

**不可重复读**

对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（UPDATE）**操作。

**幻读**

幻读是针对数据**插入（INSERT）**操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。

从上往下，隔离强度逐渐增强，性能逐渐变差。采用哪种隔离级别要根据系统需求权衡决定，其中，**可重复读**是 MySQL 的默认级别。

事务隔离其实就是为了解决上面提到的脏读、不可重复读、幻读这几个问题，下面展示了 4 种隔离级别对这三个问题的解决程度。

![img](https://pic4.zhimg.com/80/v2-2e1a7203478165890e2d09f36cb39857_720w.jpg)

只有串行化的隔离级别解决了全部这 3 个问题，其他的 3 个隔离级别都有缺陷。

加锁的过程要分有索引和无索引两种情况，比如下面这条语句

```mysql
update user set age=11 where id = 1
```

id 是这张表的主键，是有索引的情况，那么 MySQL 直接就在索引数中找到了这行数据，然后干净利落的加上行锁就可以了。

而下面这条语句

```mysql
update user set age=11 where age=10
```

<u>表中并没有为 age 字段设置索引，所以， MySQL 无法直接定位到这行数据。那怎么办呢，当然也不是加表锁了。MySQL 会为这张表中所有行加行锁，没错，是所有行。但是呢，在加上行锁后，MySQL 会进行一遍过滤，发现不满足的行就释放锁，最终只留下符合条件的行</u>。<u>虽然最终只为符合条件的行加了锁，但是这一锁一释放的过程对性能也是影响极大的</u>。所以，如果是大表的话，建议合理设计索引，如果真的出现这种情况，那很难保证并发度。

上面介绍可重复读的时候，那张图里标示着出现幻读的地方实际上在 MySQL 中并不会出现，MySQL 已经在可重复读隔离级别下解决了幻读的问题。

前面刚说了并发写问题的解决方式就是行锁，而解决幻读用的也是锁，叫做间隙锁，MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。

假设现在表中有两条记录，并且 age 字段已经添加了索引，两条记录 age 的值分别为 10 和 30。

![img](https://pic3.zhimg.com/80/v2-6a7c4b03297309813d57213a2a2d7366_720w.jpg)

<u>此时，在数据库中会为索引维护一套B+树，用来快速定位行记录。B+索引树是有序的，所以会把这张表的索引分割成几个区间。</u>

![img](https://pic1.zhimg.com/80/v2-f07516da40274690e8ece9bc415eab88_720w.jpg)

如图所示，分成了3 个区间，(负无穷,10]、(10,30]、(30,正无穷]，在这3个区间是可以加间隙锁的。

之后，我用下面的两个事务演示一下加锁过程。

![img](https://pic2.zhimg.com/80/v2-3ce9ac24fb6fdf3ff60d03e070136175_720w.jpg)

在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。当事务A执行`update user set name='风筝2号’ where age = 10;` 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。不仅插入 age = 10 的记录需要等待事务A提交，age<10、10<age<30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。

这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入。



## **总结**

MySQL 的 InnoDB 引擎才支持事务，其中可重复读是默认的隔离级别。

读未提交和串行化基本上是不需要考虑的隔离级别，前者不加锁限制，后者相当于单线程执行，效率太差。

读提交解决了脏读问题，行锁解决了并发更新的问题。并且 MySQL 在可重复读级别解决了幻读问题，是通过行锁和间隙锁的组合 Next-Key 锁实现的。



# MySQL 索引面试题

## **优点**

- 索引大大减小了服务器需要扫描的数据量

- 索引可以帮助服务器避免排序和临时表

- 索引可以将随机IO变成顺序IO

- 索引对于InnoDB（对索引支持行级锁）非常重要，因为它可以让查询锁更少的元组。在MySQL5.1和更新的版本中，InnoDB可以在服务器端过滤掉行后就释放锁，但在早期的MySQL版本中，InnoDB直到事务提交时才会解锁。对不需要的元组的加锁，会增加锁的开销，降低并发性。 InnoDB仅对需要访问的元组加锁，而索引能够减少InnoDB访问的元组数。但是只有在存储引擎层过滤掉那些不需要的数据才能达到这种目的。一旦索引不允许InnoDB那样做（即索引达不到过滤的目的），MySQL服务器只能对InnoDB返回的数据进行WHERE操作，此时，已经无法避免对那些元组加锁了。如果查询不能使用索引，MySQL会进行全表扫描，并锁住每一个元组，不管是否真正需要。

- - 关于InnoDB、索引和锁：InnoDB在二级索引上使用共享锁（读锁），但访问主键索引需要排他锁（写锁）

## **缺点**

- 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存索引文件。
- 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。
- 如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果。
- 对于非常小的表，大部分情况下简单的全表扫描更高效；

索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。

因此应该只为最经常查询和最经常排序的数据列建立索引。

<u>MySQL里同一个数据表里的索引总数限制为16个。</u>

## 索引存储类型

## B-Tree索引

https://zhuanlan.zhihu.com/p/29118331

https://www.cnblogs.com/JasonCeng/p/12044109.html

InnoDB使用的是B+Tree。
B+Tree：每一个叶子节点都包含指向下一个叶子节点的指针，从而方便叶子节点的范围遍历。

B-Tree通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同，很适合查找范围数据。

B-Tree可以对<，<=，=，>，>=，BETWEEN，IN，以及不以通配符开始的LIKE使用索引。



## 索引查询

可以利用B-Tree索引进行全关键字、关键字范围和关键字前缀查询，但必须保证按索引的最左边前缀(leftmost prefix of the index)来进行查询。

![img](https://pic3.zhimg.com/80/v2-307ae0886144c7bc2619691ff616a6a2_720w.jpg)

假设有如下一个表：

```text
CREATE TABLE People (
  last_name varchar(50) not null,
  first_name varchar(50) not null,
  dob date not null,
  gender enum('m', 'f') not null,
  key(last_name, first_name, dob)
);
```

其组合索引包含表中每一行的last_name、first_name和dob列。其结构大致如下：

![img](https://pic3.zhimg.com/80/v2-a0669007f11ae9e57bd64394295f84aa_720w.jpg)

按索引的最左边前缀(leftmost prefix of the index)来进行查询：

1. 查询必须从索引的最左边的列开始，否则无法使用索引。例如，你不能直接利用索引查找在某一天出生的人。
2. 不能跳过某一索引列。例如，你不能利用索引查找last name为Smith且出生于某一天的人。
3. 存储引擎不能使用索引中范围条件右边的列。例如，如果你的查询语句为WHERE last_name="Smith" AND first_name LIKE 'J%' AND dob='1976-12-23'，则该查询只会使用索引中的前两列，因为LIKE是范围查询。



1. 匹配全值(Match the full value)：对索引中的所有列都指定具体的值。例如，上图中索引可以帮助你查找出生于1960-01-01的Cuba Allen。
2. 匹配最左前缀(Match a leftmost prefix)：你可以利用索引查找last name为Allen的人，仅仅使用索引中的第1列。
3. 匹配列前缀(Match a column prefix)：例如，你可以利用索引查找last name以J开始的人，这仅仅使用索引中的第1列。
4. 匹配值的范围查询(Match a range of values)：可以利用索引查找last name在Allen和Barrymore之间的人，仅仅使用索引中第1列。
5. 匹配部分精确而其它部分进行范围匹配(Match one part exactly and match a range on another part)：可以利用索引查找last name为Allen，而first name以字母K开始的人。
6. 仅对索引进行查询(Index-only queries)：如果查询的列都位于索引中，则不需要再多一次I/O回读元组。(覆盖索引：索引的叶子节点中已经包含要查询的数据，那么就没有必要再回表查询了，<u>如果索引包含满足查询的所有数据，就称为覆盖索引。</u>) select中查询的列都建立了索引。

## **聚簇索引(cluster index)**

一个表只能有一个聚簇索引。

目前，只有solidDB和InnoDB支持聚簇索引，MyISAM不支持聚簇索引。一些DBMS允许用户指定聚簇索引，但是MySQL的存储引擎到目前为止都不支持。

## **InnoDB的聚簇索引：**

1. InnoDB对主键建立聚簇索引。
2. 如果你不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。
3. 如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后对其建立聚簇索引。

InnoDB默认使用聚簇索引来组织数据，如果你用InnoDB，而且不需要特殊的聚簇索引，一个好的做法就是使用代理主键(surrogate key)——独立于你的应用中的数据。最简单的做法就是使用一个AUTO_INCREMENT的列，这会保证记录按照顺序插入，而且能提高使用primary key进行连接的查询的性能。应该尽量避免随机的聚簇主键，例如字符串主键就是一个不好的选择，它使得插入操作变得随机。

一般来说，DBMS都会以聚簇索引的形式来存储实际的数据，它是其它二级索引的基础：

- 聚簇索引（primary索引）：主键索引
- 非聚簇索引（second索引）：二级索引



索引分类有：

1. B-Tree索引
2. 聚簇索引
3. 二级索引
4. Hash索引
5. 空间(R-Tree)索引
6. 全文(Full-Text)索引



索引类型：

1. 普通索引
2. 唯一索引
3. 主键索引
4. 组合索引



# Kafka

生产者采用 push 模式将数据发布到 broker，每条消息追加到分区中，顺序写入磁盘。消息写入 Leader 后，Follower 是主动与 Leader 进行同步。

Kafka 消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过 producer.type 属性进行配置。

Kafka 通过配置 request.required.acks 属性来确认 Producer 的消息：

- 0：表示不进行消息接收是否成功的确认；不能保证消息是否发送成功，生成环境基本不会用。
- 1：默认值，表示当 Leader 接收成功时确认；只要 Leader 存活就可以保证不丢失，保证了吞吐量。所以默认的 producer 级别是 at least once。
- all：保证 leader 和 follower 不丢，但是如果网络拥塞，没有收到 ACK，会有重复发的问题。

如果 acks 配置为 0，发生网络抖动消息丢了，生产者不校验 ACK 自然就不知道丢了。

如果 acks 配置为 1 保证 leader 不丢，但是如果 leader 挂了，恰好选了一个没有 ACK 的 follower，那也丢了。

如果 acks 配置为 all 保证 leader 和 follower 不丢，但是如果网络拥塞，没有收到 ACK，会有重复发的问题。